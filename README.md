
#  \[AAAI 2025\] Identity-Text-Video-Corpus-Grounding  [\[Paper\]](https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2025_Identity-Text%20Video%20Corpus%20Grounding.pdf)

Official PyTorch implementation of the paper "Identity-Text Video Corpus Grounding". 




## News ðŸš€ðŸš€ðŸš€

- `2025/03/07`: ðŸ”¥ We release the code of [Video-Locator](model).
- `2025/01/15`: ðŸ”¥ We release the [TVR-IT data](data).



## License

<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a> 

The code is released under the [CC-BY-NC-ND license](LICENSE). 

The TVR-IT dataset is for research and non-commercial use only. The annotations are derived from the [original TVR dataset](https://github.com/jayleicn/TVRetrieval/tree/master/data), with all copyrights belonging to the original authors. The images were collected from public sources via Google Search, with all rights reserved by their respective copyright holders.


## Citation

If you find this project useful in your research, please consider cite:

```BibTeX
@article{huang2025identity,
  title={Identity-Text Video Corpus Grounding},
  author={Huang, Bin and Wang, Xin and Chen, Hong and Chen, Houlun and Wu, Yaofei and Zhu, Wenwu},
  year={2025}
}
```


## Acknowledgement

Video-Locator is built with reference to the code of the following projects: [OpenAI CLIP](https://github.com/openai/CLIP), [LLaVA](https://github.com/haotian-liu/LLaVA) and [VTimeLLM](https://github.com/huangb23/VTimeLLM/). Thanks for their awesome work!


Looking forward to your feedback, contributions, and stars! ðŸŒŸ